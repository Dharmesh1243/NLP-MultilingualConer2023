{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Mount Google Drive to access data files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guRilQZo5RGj",
        "outputId": "5b524df1-3c29-4fb2-ed42-195a73c8b643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers[torch]\n",
        "!pip install datasets\n",
        "!pip install tqdm\n",
        "!pip install seqeval\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s53jYUGu5ToD",
        "outputId": "ac053d47-e183-4961-8127-8fb6643390e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, accelerate\n",
            "Successfully installed accelerate-0.22.0 huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=51215b1b4248fdf75dc7f96f13532bab882b033208f05e71900db4c96eda41a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning, module=\"seqeval\")\n",
        "from transformers import RemBertTokenizerFast, RemBertForTokenClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "import gc\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hkx55ngr5W39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr4jbfMZ5Jkh"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Define a dictionary for grouping labels into broader categories\n",
        "# Label Grouping dictionary\n",
        "labels = {\n",
        "    'B-Disease': 'B-Medical',\n",
        "    'I-Disease': 'I-Medical',\n",
        "    'B-Symptom': 'B-Medical',\n",
        "    'I-Symptom': 'I-Medical',\n",
        "    'B-AnatomicalStructure': 'B-Medical',\n",
        "    'I-AnatomicalStructure': 'I-Medical',\n",
        "    'B-MedicalProcedure': 'B-Medical',\n",
        "    'I-MedicalProcedure': 'I-Medical',\n",
        "    'B-Medication/Vaccine': 'B-Medical',\n",
        "    'I-Medication/Vaccine': 'I-Medical',\n",
        "\n",
        "    'B-OtherPROD': 'B-Product',\n",
        "    'I-OtherPROD': 'I-Product',\n",
        "    'B-Drink': 'B-Product',\n",
        "    'I-Drink': 'I-Product',\n",
        "    'B-Food': 'B-Product',\n",
        "    'I-Food': 'I-Product',\n",
        "    'B-Vehicle': 'B-Product',\n",
        "    'I-Vehicle': 'I-Product',\n",
        "    'B-Clothing': 'B-Product',\n",
        "    'I-Clothing': 'I-Product',\n",
        "\n",
        "    'B-OtherPER': 'B-Person',\n",
        "    'I-OtherPER': 'I-Person',\n",
        "    'B-SportsManager': 'B-Person',\n",
        "    'I-SportsManager': 'I-Person',\n",
        "    'B-Cleric': 'B-Person',\n",
        "    'I-Cleric': 'I-Person',\n",
        "    'B-Politician': 'B-Person',\n",
        "    'I-Politician': 'I-Person',\n",
        "    'B-Athlete': 'B-Person',\n",
        "    'I-Athlete': 'I-Person',\n",
        "    'B-Artist': 'B-Person',\n",
        "    'I-Artist': 'I-Person',\n",
        "    'B-Scientist': 'B-Person',\n",
        "    'I-Scientist': 'I-Person',\n",
        "\n",
        "    'B-MusicalGRP': 'B-Group',\n",
        "    'I-MusicalGRP': 'I-Group',\n",
        "    'B-PublicCorp': 'B-Group',\n",
        "    'I-PublicCorp': 'I-Group',\n",
        "    'B-PrivateCorp': 'B-Group',\n",
        "    'I-PrivateCorp': 'I-Group',\n",
        "    'B-AerospaceManufacturer': 'B-Group',\n",
        "    'I-AerospaceManufacturer': 'I-Group',\n",
        "    'B-SportsGRP': 'B-Group',\n",
        "    'I-SportsGRP': 'I-Group',\n",
        "    'B-CarManufacturer': 'B-Group',\n",
        "    'I-CarManufacturer': 'I-Group',\n",
        "    'B-ORG': 'B-Group',\n",
        "    'I-ORG': 'I-Group',\n",
        "\n",
        "    'B-VisualWork': 'B-CW',\n",
        "    'I-VisualWork': 'I-CW',\n",
        "    'B-MusicalWork': 'B-CW',\n",
        "    'I-MusicalWork': 'I-CW',\n",
        "    'B-WrittenWork': 'B-CW',\n",
        "    'I-WrittenWork': 'I-CW',\n",
        "    'B-ArtWork': 'B-CW',\n",
        "    'I-ArtWork': 'I-CW',\n",
        "    'B-Software': 'B-CW',\n",
        "    'I-Software': 'I-CW',\n",
        "\n",
        "    'B-Facility': 'B-Location',\n",
        "    'I-Facility': 'I-Location',\n",
        "    'B-OtherLOC': 'B-Location',\n",
        "    'I-OtherLOC': 'I-Location',\n",
        "    'B-HumanSettlement': 'B-Location',\n",
        "    'I-HumanSettlement': 'I-Location',\n",
        "    'B-Station': 'B-Location',\n",
        "    'I-Station': 'I-Location',\n",
        "\n",
        "    'O': 'O'\n",
        "}\n",
        "\n",
        "\n",
        "def convert_to_general_label(label):\n",
        "    return labels.get(label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the perdiction files for 12 languages , individually. Due to High ram usage of the ram usage, this cannot be be done at single turn. the languages the ensemble model support are below.\n",
        "\n"
      ],
      "metadata": {
        "id": "SVf1NHHcBqrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "#languages = ['EN-English', 'ES-Spanish', 'HI-Hindi', 'ZH-Chinese', 'SV-Swedish', 'FA-Farsi', 'FR-French', 'IT-Italian', 'PT-Portuguese', 'UK-Ukrainian', 'DE-German', 'BN-Bangla']\n",
        "languages = ['DE-German']\n",
        "predictions_base_path = '/content/drive/MyDrive/predictions'\n",
        "\n",
        "# Create variables to store the predictions and label IDs for each language and model\n",
        "predictions = {}\n",
        "label_ids = {}\n",
        "\n",
        "# Load the predictions and label IDs for each language\n",
        "for lang in languages:\n",
        "    # Load XLM-R predictions and label IDs\n",
        "    print(f'Loading XLM-R predictions and label IDs for {lang}')\n",
        "    xlmr_prediction_file_path = os.path.join(predictions_base_path, f'xlmr_{lang}_predictions.pkl')\n",
        "    with open(xlmr_prediction_file_path, 'rb') as f:\n",
        "        predictions[f'xlmr_{lang}'], label_ids[f'xlmr_{lang}'] = pickle.load(f)\n",
        "\n",
        "    # Load mBERT predictions and label IDs\n",
        "    print(f'Loading mBERT predictions and label IDs for {lang}')\n",
        "    mbert_prediction_file_path = os.path.join(predictions_base_path, f'mbert_{lang}_predictions.pkl')\n",
        "    with open(mbert_prediction_file_path, 'rb') as f:\n",
        "        predictions[f'mbert_{lang}'], label_ids[f'mbert_{lang}'] = pickle.load(f)\n",
        "\n",
        "    # Load RemBERT predictions and label IDs\n",
        "    print(f'Loading RemBERT predictions and label IDs for {lang}')\n",
        "    rembert_prediction_file_path = os.path.join(predictions_base_path, f'RemBert_{lang}_predictions.pkl')\n",
        "    with open(rembert_prediction_file_path, 'rb') as f:\n",
        "        predictions[f'rembert_{lang}'], label_ids[f'rembert_{lang}'] = pickle.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSWCjpxaJSyW",
        "outputId": "f11f8684-29e9-4ca4-bba0-e949d6d5bb99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading XLM-R predictions and label IDs for DE-German\n",
            "Loading mBERT predictions and label IDs for DE-German\n",
            "Loading RemBERT predictions and label IDs for DE-German\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To correct the problems with inconsistency with the lables while training, weightage is chosen as per evaluation on the test datasets for individual 36 models.For each models , the weights should be chosen in the range beetween **0.xx to 1.00**."
      ],
      "metadata": {
        "id": "owBe47Qi_RgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_lengths = {}\n",
        "for lang in languages:\n",
        "    max_seq_lengths[lang] = max(predictions[f'xlmr_{lang}'].shape[1], predictions[f'mbert_{lang}'].shape[1], predictions[f'rembert_{lang}'].shape[1])\n",
        "\n",
        "# Update max_sequence_length to be the maximum of all maximum sequence lengths\n",
        "max_sequence_length = max(max_seq_lengths.values())\n",
        "print(max_sequence_length)\n",
        "# Define the weightage for each model ()NOte:the total weight should be 1\n",
        "weights = {\n",
        "    'xlmr': 0.2,  # Decreased weight for XLM-R\n",
        "    'mbert':0.3,  # Increased weight for mBERT\n",
        "    'rembert':0.5,  # Increased weight for RoBERTa-m\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize an empty list to store the ensembled logits\n",
        "ensemble_logits = []\n",
        "\n",
        "# Iterate through the predictions for a specific language\n",
        "for lang in languages:\n",
        "    # Get the logits for each model\n",
        "    xlmr_logits = predictions[f'xlmr_{lang}']\n",
        "    mbert_logits = predictions[f'mbert_{lang}']\n",
        "    rembert_logits = predictions[f'rembert_{lang}']\n",
        "\n",
        "    # Ensure that all logits have the same shape (padding or truncating)\n",
        "    common_shape = (xlmr_logits.shape[0], max_sequence_length, xlmr_logits.shape[2])\n",
        "    xlmr_logits = np.pad(xlmr_logits, ((0, 0), (0, max_sequence_length - xlmr_logits.shape[1]), (0, 0)), mode='constant')\n",
        "    mbert_logits = np.pad(mbert_logits, ((0, 0), (0, max_sequence_length - mbert_logits.shape[1]), (0, 0)), mode='constant')\n",
        "    rembert_logits = np.pad(rembert_logits, ((0, 0), (0, max_sequence_length - rembert_logits.shape[1]), (0, 0)), mode='constant')\n",
        "\n",
        "    # Calculate the weighted average of logits for each example\n",
        "    weighted_average_logits = (\n",
        "        weights['xlmr'] * xlmr_logits +\n",
        "        weights['mbert'] * mbert_logits +\n",
        "        weights['rembert'] * rembert_logits\n",
        "    ) / (weights['xlmr'] + weights['mbert'] + weights['rembert'])\n",
        "\n",
        "    # Append the ensembled logits to the list\n",
        "    ensemble_logits.append(weighted_average_logits)\n",
        "\n",
        "# Convert the ensemble_logits list to a NumPy array\n",
        "ensemble_logits = np.array(ensemble_logits)\n",
        "\n",
        "# Now you have the ensembled logits for each example in ensemble_logits"
      ],
      "metadata": {
        "id": "r9z3Qw8WLWbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcbee51-0515-4c0b-877a-6bb6a85231e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if lang in languages:\n",
        "    # Retrieve the label_ids for the RemBERT model for the specified language\n",
        "    rembert_label_ids = label_ids[f'rembert_{lang}']\n",
        "    print(f'Label IDs for RemBERT model for {lang}:')\n",
        "\n",
        "else:\n",
        "    print(f'{lang} is not found in the languages list.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGA3cOH_VrVA",
        "outputId": "dc3d010a-7ce6-47e6-80e3-3b4bf16b5cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label IDs for RemBERT model for DE-German:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rembert_index_to_label_base_path = '/content/drive/MyDrive/RemBert_index_to_label'\n",
        "\n",
        "rembert_index_to_label = {}\n",
        "for lang in languages:\n",
        "    # Create the RemBERT index_to_label directory for this language\n",
        "    rembert_index_to_label_dir = f'{rembert_index_to_label_base_path}/{lang}'\n",
        "    if not os.path.exists(rembert_index_to_label_dir):\n",
        "        os.makedirs(rembert_index_to_label_dir)\n",
        "\n",
        "# Initialize an empty dictionary for label_to_index mapping\n",
        "label_to_index = {}\n",
        "\n",
        "# Iterate through the existing index_to_label mapping\n",
        "for index, label in rembert_index_to_label.items():\n",
        "    # Assign the label as the key and the index as the value\n",
        "    label_to_index[label] = index\n"
      ],
      "metadata": {
        "id": "Nxm8PPCtXOa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Master Labelling consists three options. Choose from the List below:\n",
        "\n",
        "**rembert**,\n",
        "**xlmr**,\n",
        "**mbert**"
      ],
      "metadata": {
        "id": "J8pP00FFE170"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''master_label_mapping={'xlmr': {'I-Cleric': 0, 'I-Artist': 1, 'B-AerospaceManufacturer': 2, 'B-Artist': 3, 'B-MusicalWork': 4, 'I-Disease': 5, 'I-Software': 6, 'I-PrivateCorp': 7, 'B-OtherLOC': 8, 'I-AerospaceManufacturer': 9, 'I-HumanSettlement': 10, 'B-Clothing': 11, 'I-Facility': 12, 'I-VisualWork': 13, 'I-Station': 14, 'I-OtherPROD': 15, 'B-MedicalProcedure': 16, 'I-MusicalWork': 17, 'I-AnatomicalStructure': 18, 'B-ArtWork': 19, 'B-Athlete': 20, 'B-Station': 21, 'B-Food': 22, 'B-AnatomicalStructure': 23, 'B-OtherPER': 24, 'B-HumanSettlement': 25, 'I-OtherLOC': 26, 'B-Scientist': 27, 'I-Politician': 28, 'B-CarManufacturer': 29, 'B-OtherPROD': 30, 'I-Food': 31, 'B-SportsManager': 32, 'B-Cleric': 33, 'I-WrittenWork': 34, 'I-Clothing': 35, 'B-PrivateCorp': 36, 'B-Politician': 37, 'B-Facility': 38, 'I-SportsManager': 39, 'B-PublicCorp': 40, 'I-PublicCorp': 41, 'I-Symptom': 42, 'I-Scientist': 43, 'B-Software': 44, 'B-Vehicle': 45, 'I-Medication/Vaccine': 46, 'I-ORG': 47, 'B-MusicalGRP': 48, 'I-MusicalGRP': 49, 'I-Drink': 50, 'B-ORG': 51, 'I-SportsGRP': 52, 'B-VisualWork': 53, 'I-Athlete': 54, 'B-Medication/Vaccine': 55, 'B-Symptom': 56, 'I-MedicalProcedure': 57, 'B-Disease': 58, 'I-OtherPER': 59, 'I-ArtWork': 60, 'B-WrittenWork': 61, 'B-SportsGRP': 62, 'I-CarManufacturer': 63, 'I-Vehicle': 64, 'O': 65, 'B-Drink': 66}, 'mbert': {'I-CarManufacturer': 0, 'I-Disease': 1, 'B-Athlete': 2, 'B-Station': 3, 'I-AerospaceManufacturer': 4, 'I-Vehicle': 5, 'I-Symptom': 6, 'B-MusicalGRP': 7, 'I-HumanSettlement': 8, 'I-MusicalWork': 9, 'I-Facility': 10, 'B-SportsGRP': 11, 'B-Food': 12, 'I-PrivateCorp': 13, 'I-Artist': 14, 'B-WrittenWork': 15, 'I-SportsManager': 16, 'B-Software': 17, 'B-PublicCorp': 18, 'O': 19, 'B-Disease': 20, 'B-Artist': 21, 'I-Cleric': 22, 'B-OtherPROD': 23, 'I-Medication/Vaccine': 24, 'I-MusicalGRP': 25, 'I-Software': 26, 'I-Drink': 27, 'I-OtherPER': 28, 'B-CarManufacturer': 29, 'I-Clothing': 30, 'B-Clothing': 31, 'B-MedicalProcedure': 32, 'I-OtherLOC': 33, 'B-Symptom': 34, 'B-Vehicle': 35, 'B-ArtWork': 36, 'B-ORG': 37, 'I-Athlete': 38, 'I-OtherPROD': 39, 'B-VisualWork': 40, 'B-Drink': 41, 'I-AnatomicalStructure': 42, 'B-Scientist': 43, 'I-WrittenWork': 44, 'B-Facility': 45, 'B-Cleric': 46, 'B-Politician': 47, 'B-PrivateCorp': 48, 'B-SportsManager': 49, 'I-MedicalProcedure': 50, 'B-HumanSettlement': 51, 'I-Scientist': 52, 'I-Station': 53, 'I-SportsGRP': 54, 'B-Medication/Vaccine': 55, 'B-AnatomicalStructure': 56, 'B-OtherLOC': 57, 'B-MusicalWork': 58, 'I-ORG': 59, 'I-VisualWork': 60, 'I-Food': 61, 'I-Politician': 62, 'I-PublicCorp': 63, 'B-AerospaceManufacturer': 64, 'I-ArtWork': 65, 'B-OtherPER': 66}, 'rembert': {'I-MusicalGRP': 0, 'I-PrivateCorp': 1, 'I-SportsGRP': 2, 'B-Station': 3, 'B-Drink': 4, 'I-Artist': 5, 'B-Athlete': 6, 'I-Software': 7, 'I-Vehicle': 8, 'B-CarManufacturer': 9, 'I-AnatomicalStructure': 10, 'B-OtherPROD': 11, 'O': 12, 'B-MusicalGRP': 13, 'B-PublicCorp': 14, 'I-CarManufacturer': 15, 'I-WrittenWork': 16, 'B-OtherLOC': 17, 'I-Politician': 18, 'I-Symptom': 19, 'B-Scientist': 20, 'I-Medication/Vaccine': 21, 'B-Food': 22, 'B-SportsManager': 23, 'B-AnatomicalStructure': 24, 'I-Station': 25, 'B-ArtWork': 26, 'I-ArtWork': 27, 'B-PrivateCorp': 28, 'I-AerospaceManufacturer': 29, 'B-MedicalProcedure': 30, 'B-Artist': 31, 'I-PublicCorp': 32, 'I-OtherLOC': 33, 'I-MedicalProcedure': 34, 'I-OtherPER': 35, 'B-Facility': 36, 'I-HumanSettlement': 37, 'I-OtherPROD': 38, 'B-Clothing': 39, 'I-VisualWork': 40, 'B-VisualWork': 41, 'B-Software': 42, 'B-Disease': 43, 'I-ORG': 44, 'B-SportsGRP': 45, 'I-Food': 46, 'I-SportsManager': 47, 'I-Clothing': 48, 'B-MusicalWork': 49, 'B-AerospaceManufacturer': 50, 'I-Cleric': 51, 'B-Medication/Vaccine': 52, 'B-HumanSettlement': 53, 'B-Politician': 54, 'B-Vehicle': 55, 'I-Athlete': 56, 'B-ORG': 57, 'B-Symptom': 58, 'I-Facility': 59, 'I-MusicalWork': 60, 'I-Disease': 61, 'B-WrittenWork': 62, 'B-OtherPER': 63, 'I-Drink': 64, 'I-Scientist': 65, 'B-Cleric': 66}}\n",
        "label_mapping = master_label_mapping['rembert']\n",
        "\n",
        "# Create the index_to_label dictionary for the RemBERT model\n",
        "index_to_label = {i: label for label, i in label_mapping.items()}\n",
        "'''\n",
        "master_label_mapping={\n",
        "  'I-CarManufacturer': 0,\n",
        "  'I-Disease': 1,\n",
        "  'B-Athlete': 2,\n",
        "  'B-Station': 3,\n",
        "  'I-AerospaceManufacturer': 4,\n",
        "  'I-Vehicle': 5,\n",
        "  'I-Symptom': 6,\n",
        "  'B-MusicalGRP': 7,\n",
        "  'I-HumanSettlement': 8,\n",
        "  'I-MusicalWork': 9,\n",
        "  'I-Facility': 10,\n",
        "  'B-SportsGRP': 11,\n",
        "  'B-Food': 12,\n",
        "  'I-PrivateCorp': 13,\n",
        "  'I-Artist': 14,\n",
        "  'B-WrittenWork': 15,\n",
        "  'I-SportsManager': 16,\n",
        "  'B-Software': 17,\n",
        "  'B-PublicCorp': 18,\n",
        "  'O': 19,\n",
        "  'B-Disease': 20,\n",
        "  'B-Artist': 21,\n",
        "  'I-Cleric': 22,\n",
        "  'B-OtherPROD': 23,\n",
        "  'I-Medication/Vaccine': 24,\n",
        "  'I-MusicalGRP': 25,\n",
        "  'I-Software': 26,\n",
        "  'I-Drink': 27,\n",
        "  'I-OtherPER': 28,\n",
        "  'B-CarManufacturer': 29,\n",
        "  'I-Clothing': 30,\n",
        "  'B-Clothing': 31,\n",
        "  'B-MedicalProcedure': 32,\n",
        "  'I-OtherLOC': 33,\n",
        "  'B-Symptom': 34,\n",
        "  'B-Vehicle': 35,\n",
        "  'B-ArtWork': 36,\n",
        "  'B-ORG': 37,\n",
        "  'I-Athlete': 38,\n",
        "  'I-OtherPROD': 39,\n",
        "  'B-VisualWork': 40,\n",
        "  'B-Drink': 41,\n",
        "  'I-AnatomicalStructure': 42,\n",
        "  'B-Scientist': 43,\n",
        "  'I-WrittenWork': 44,\n",
        "  'B-Facility': 45,\n",
        "  'B-Cleric': 46,\n",
        "  'B-Politician': 47,\n",
        "  'B-PrivateCorp': 48,\n",
        "  'B-SportsManager': 49,\n",
        "  'I-MedicalProcedure': 50,\n",
        "  'B-HumanSettlement': 51,\n",
        "  'I-Scientist': 52,\n",
        "  'I-Station': 53,\n",
        "  'I-SportsGRP': 54,\n",
        "  'B-Medication/Vaccine': 55,\n",
        "  'B-AnatomicalStructure': 56,\n",
        "  'B-OtherLOC': 57,\n",
        "  'B-MusicalWork': 58,\n",
        "  'I-ORG': 59,\n",
        "  'I-VisualWork': 60,\n",
        "  'I-Food': 61,\n",
        "  'I-Politician': 62,\n",
        "  'I-PublicCorp': 63,\n",
        "  'B-AerospaceManufacturer': 64,\n",
        "  'I-ArtWork': 65,\n",
        "  'B-OtherPER': 66\n",
        "}\n",
        "index_to_label = {i: label for label, i in master_label_mapping.items()}"
      ],
      "metadata": {
        "id": "jFXqFY2UmbGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_metrics(p):\n",
        "    predictions = np.argmax(p.predictions, axis=-1)\n",
        "    true_labels = p.label_ids\n",
        "    true_label_list = [[] for _ in range(true_labels.shape[0])]\n",
        "    pred_label_list = [[] for _ in range(true_labels.shape[0])]\n",
        "\n",
        "    for i in range(true_labels.shape[0]):\n",
        "        for j in range(true_labels.shape[1]):\n",
        "            if true_labels[i, j] != -100:\n",
        "                true_label_index = int(true_labels[i, j])\n",
        "                pred_label_index = int(predictions[0, i, j])\n",
        "                true_label = index_to_label.get(true_label_index, 'UNKNOWN_LABEL')\n",
        "                pred_label = index_to_label.get(pred_label_index, 'UNKNOWN_LABEL')\n",
        "                true_label_list[i].append(true_label)\n",
        "                pred_label_list[i].append(pred_label)\n",
        "\n",
        "        true_label_list[i] = [convert_to_general_label(label) for label in true_label_list[i]]\n",
        "        pred_label_list[i] = [convert_to_general_label(label) for label in pred_label_list[i]]\n",
        "\n",
        "    results = {\n",
        "        \"precision\": precision_score(true_label_list, pred_label_list),\n",
        "        \"recall\": recall_score(true_label_list, pred_label_list),\n",
        "        \"f1\": f1_score(true_label_list, pred_label_list),\n",
        "    }\n",
        "\n",
        "    report = classification_report(true_label_list, pred_label_list)\n",
        "    print(report)\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "ShlGViRGWuNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new object to hold the predictions and label IDs\n",
        "#print(ensemble_logits.shape)\n",
        "print(f'results for {lang}')\n",
        "p = type('', (), {})()\n",
        "p.predictions = ensemble_logits\n",
        "p.label_ids = rembert_label_ids\n",
        "\n",
        "# Call the compute_metrics function with the new object as input\n",
        "# Call the compute_metrics function with the new object and rembert_label_mapping as input\n",
        "# Get the label mapping for the RemBERT model\n",
        "\n",
        "\n",
        "results = compute_metrics(p)\n",
        "\n",
        "# Print the results\n",
        "print(f'Precision: {results[\"precision\"]}')\n",
        "print(f'Recall: {results[\"recall\"]}')\n",
        "print(f'F1 Score: {results[\"f1\"]}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUjKoPuYZhWn",
        "outputId": "66a1074c-e652-460e-ae2d-feccd3e71630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results for DE-German\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CW       0.98      0.97      0.98      7450\n",
            "       Group       0.93      0.92      0.93     10068\n",
            "    Location       0.98      0.98      0.98      7165\n",
            "     Medical       0.94      0.92      0.93      7274\n",
            "      Person       0.97      0.95      0.96      6577\n",
            "     Product       0.99      1.00      1.00    239965\n",
            "\n",
            "   micro avg       0.99      0.99      0.99    278499\n",
            "   macro avg       0.97      0.96      0.96    278499\n",
            "weighted avg       0.99      0.99      0.99    278499\n",
            "\n",
            "Precision: 0.989595080919167\n",
            "Recall: 0.9893356888175541\n",
            "F1 Score: 0.9894653678682048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# RESULTS:\n",
        "#1.English\n",
        "results for EN-English\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.92      0.89      0.91     87937\n",
        "       Group       0.92      0.92      0.92    565500\n",
        "    Location       0.91      0.88      0.90     62901\n",
        "     Medical       0.89      0.88      0.89     87504\n",
        "      Person       0.94      0.93      0.94    232998\n",
        "     Product       0.85      0.86      0.85    131721\n",
        "\n",
        "   micro avg       0.91      0.91      0.91   1168561\n",
        "   macro avg       0.91      0.90      0.90   1168561\n",
        "weighted avg       0.91      0.91      0.91   1168561\n",
        "\n",
        "Precision: 0.9146125065338799\n",
        "Recall: 0.9103914986038384\n",
        "F1 Score: 0.9124971212349291\n",
        "\n",
        "#2.Spanish\n",
        "results for ES-Spanish\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.92      0.94      0.93    116808\n",
        "       Group       0.93      0.92      0.92    127220\n",
        "    Location       1.00      1.00      1.00   3242720\n",
        "     Medical       0.93      0.90      0.92     61915\n",
        "      Person       0.94      0.92      0.93    104692\n",
        "     Product       0.93      0.93      0.93    154782\n",
        "\n",
        "   micro avg       0.99      0.99      0.99   3808137\n",
        "   macro avg       0.94      0.93      0.94   3808137\n",
        "weighted avg       0.99      0.99      0.99   3808137\n",
        "\n",
        "Precision: 0.9858252670364994\n",
        "Recall: 0.9859836975402934\n",
        "F1 Score: 0.9859044759236254\n",
        "#3.Hindi\n",
        "results for HI-Hindi\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.99      0.99      0.99      7917\n",
        "       Group       0.99      0.99      0.99      7947\n",
        "    Location       0.99      0.99      0.99      4103\n",
        "     Medical       0.98      0.98      0.98      5005\n",
        "      Person       0.99      0.99      0.99     36964\n",
        "     Product       0.99      0.99      0.99      8342\n",
        "\n",
        "   micro avg       0.99      0.99      0.99     70278\n",
        "   macro avg       0.99      0.99      0.99     70278\n",
        "weighted avg       0.99      0.99      0.99     70278\n",
        "\n",
        "Precision: 0.9905342120621183\n",
        "Recall: 0.9901818492273542\n",
        "F1 Score: 0.9903579993026449\n",
        "#4.Bangla\n",
        "results for BN-Bangla\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.98      0.98      0.98      5313\n",
        "       Group       0.97      0.96      0.97      8089\n",
        "    Location       0.99      0.99      0.99      8808\n",
        "     Medical       0.97      0.96      0.96      4598\n",
        "      Person       0.98      0.98      0.98      5896\n",
        "     Product       1.00      1.00      1.00    215260\n",
        "\n",
        "   micro avg       1.00      1.00      1.00    247964\n",
        "   macro avg       0.98      0.98      0.98    247964\n",
        "weighted avg       1.00      1.00      1.00    247964\n",
        "\n",
        "Precision: 0.9952894678469884\n",
        "Recall: 0.9952533432272427\n",
        "F1 Score: 0.9952714052093184\n",
        "#5.Chinese\n",
        "results for ZH-Chinese\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.95      0.95      0.95     11413\n",
        "       Group       0.94      0.93      0.94     29888\n",
        "    Location       0.96      0.94      0.95     10442\n",
        "     Medical       0.92      0.93      0.93     16904\n",
        "      Person       0.95      0.94      0.95     13149\n",
        "     Product       0.99      0.99      0.99    357222\n",
        "\n",
        "   micro avg       0.98      0.98      0.98    439018\n",
        "   macro avg       0.95      0.95      0.95    439018\n",
        "weighted avg       0.98      0.98      0.98    439018\n",
        "\n",
        "Precision: 0.9822869316563565\n",
        "Recall: 0.9819893489560793\n",
        "F1 Score: 0.9821381177647188\n",
        "#6.Swedish\n",
        "results for SV-Swedish\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.94      0.93      0.93    129935\n",
        "       Group       0.87      0.90      0.89    113751\n",
        "    Location       0.97      0.97      0.97     46131\n",
        "     Medical       0.93      0.90      0.91     94813\n",
        "      Person       0.94      0.92      0.93     65040\n",
        "     Product       0.99      1.00      1.00   2417877\n",
        "\n",
        "   micro avg       0.98      0.98      0.98   2867547\n",
        "   macro avg       0.94      0.94      0.94   2867547\n",
        "weighted avg       0.98      0.98      0.98   2867547\n",
        "\n",
        "Precision: 0.9835651665040716\n",
        "Recall: 0.9840940706464445\n",
        "F1 Score: 0.9838295474908989\n",
        "#7.Farsi\n",
        "results for FA-Farsi\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.91      0.92      0.92    108372\n",
        "       Group       0.92      0.90      0.91    106740\n",
        "    Location       0.99      0.99      0.99   3288013\n",
        "     Medical       0.94      0.93      0.93     55366\n",
        "      Person       0.93      0.90      0.92     75750\n",
        "     Product       0.91      0.91      0.91    146611\n",
        "\n",
        "   micro avg       0.98      0.98      0.98   3780852\n",
        "   macro avg       0.93      0.93      0.93   3780852\n",
        "weighted avg       0.98      0.98      0.98   3780852\n",
        "\n",
        "Precision: 0.981371741945618\n",
        "Recall: 0.9815433135176939\n",
        "F1 Score: 0.981457520233419\n",
        "#8.French\n",
        "results for FR-French\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.92      0.92      0.92    189956\n",
        "       Group       0.89      0.89      0.89    177572\n",
        "    Location       0.93      0.90      0.92     37854\n",
        "     Medical       0.90      0.88      0.89    117354\n",
        "      Person       0.91      0.88      0.90     94118\n",
        "     Product       0.99      0.99      0.99   2984667\n",
        "\n",
        "   micro avg       0.98      0.98      0.98   3601521\n",
        "   macro avg       0.92      0.91      0.92   3601521\n",
        "weighted avg       0.98      0.98      0.98   3601521\n",
        "\n",
        "Precision: 0.9764103101866801\n",
        "Recall: 0.9772634950622251\n",
        "F1 Score: 0.9768367163281452\n",
        "#9.Italian\n",
        "results for IT-Italian\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.87      0.89      0.88     77672\n",
        "       Group       0.93      0.91      0.92     96278\n",
        "    Location       0.99      0.99      0.99   2749549\n",
        "     Medical       0.95      0.96      0.96    197768\n",
        "      Person       0.97      0.97      0.97    256915\n",
        "     Product       0.92      0.89      0.91     83167\n",
        "\n",
        "   micro avg       0.98      0.98      0.98   3461349\n",
        "   macro avg       0.94      0.94      0.94   3461349\n",
        "weighted avg       0.98      0.98      0.98   3461349\n",
        "\n",
        "Precision: 0.9822461658603305\n",
        "Recall: 0.9825943584423299\n",
        "F1 Score: 0.9824202312994436\n",
        "#10.Portugese\n",
        "results for PT-Portuguese\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.88      0.91      0.89     79937\n",
        "       Group       0.93      0.90      0.92     79008\n",
        "    Location       0.99      0.99      0.99   2948519\n",
        "     Medical       0.95      0.95      0.95    162371\n",
        "      Person       0.95      0.94      0.94    151038\n",
        "     Product       0.94      0.90      0.92     87677\n",
        "\n",
        "   micro avg       0.98      0.98      0.98   3508550\n",
        "   macro avg       0.94      0.93      0.94   3508550\n",
        "weighted avg       0.98      0.98      0.98   3508550\n",
        "\n",
        "Precision: 0.983299758494776\n",
        "Recall: 0.9837254706360177\n",
        "F1 Score: 0.9835125684981618\n",
        "#11.Ukranian\n",
        "results for UK-Ukrainian\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.87      0.90      0.89     63992\n",
        "       Group       0.92      0.88      0.90     65932\n",
        "    Location       0.99      0.99      0.99   2722230\n",
        "     Medical       0.95      0.94      0.94    137036\n",
        "      Person       0.93      0.92      0.92     98315\n",
        "     Product       0.94      0.90      0.92     85602\n",
        "\n",
        "   micro avg       0.98      0.98      0.98   3173107\n",
        "   macro avg       0.93      0.92      0.93   3173107\n",
        "weighted avg       0.98      0.98      0.98   3173107\n",
        "\n",
        "Precision: 0.9828441900135161\n",
        "Recall: 0.9828943682012614\n",
        "F1 Score: 0.9828692784669549\n",
        "#12.German\n",
        "results for DE-German\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "          CW       0.98      0.97      0.98      7450\n",
        "       Group       0.93      0.92      0.93     10068\n",
        "    Location       0.98      0.98      0.98      7165\n",
        "     Medical       0.94      0.92      0.93      7274\n",
        "      Person       0.97      0.95      0.96      6577\n",
        "     Product       0.99      1.00      1.00    239965\n",
        "\n",
        "   micro avg       0.99      0.99      0.99    278499\n",
        "   macro avg       0.97      0.96      0.96    278499\n",
        "weighted avg       0.99      0.99      0.99    278499\n",
        "\n",
        "Precision: 0.989595080919167\n",
        "Recall: 0.9893356888175541\n",
        "F1 Score: 0.9894653678682048"
      ],
      "metadata": {
        "id": "iBzTqpB2Wdjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9MMwa3Y9Xvax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code to break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Nd8Ef9k1Xkaq",
        "outputId": "6c81a837-772f-48ed-8f94-774517907dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-c34cc9d91f19>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    code to break\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code for saving the Index_to Lable**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fu4k_73vl20t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Set the base paths for the tokenizers\n",
        "rembert_tokenizer_base_path = '/content/drive/MyDrive/RemBert_fin_models'\n",
        "mbert_tokenizer_base_path = '/content/drive/MyDrive/mbert_fin_models'\n",
        "xlm_tokenizer_base_path = '/content/drive/MyDrive/xlm_fin_models'\n",
        "\n",
        "# Define the list of languages\n",
        "languages = ['EN-English', 'ES-Spanish', 'HI-Hindi', 'ZH-Chinese', 'SV-Swedish', 'FA-Farsi', 'FR-French', 'IT-Italian', 'PT-Portuguese', 'UK-Ukrainian', 'DE-German', 'BN-Bangla']\n",
        "\n",
        "# Load the tokenizers for all languages\n",
        "rembert_tokenizers = {}\n",
        "mbert_tokenizers = {}\n",
        "xlm_tokenizers = {}\n",
        "for lang in languages:\n",
        "    # Load the RemBERT tokenizer for this language\n",
        "    rembert_tokenizer_path = f'{rembert_tokenizer_base_path}/{lang}/tokenizer/tokenizer.pkl'\n",
        "    with open(rembert_tokenizer_path, 'rb') as f:\n",
        "        rembert_tokenizers[lang] = pickle.load(f)\n",
        "\n",
        "    # Load the mBERT tokenizer for this language\n",
        "    mbert_tokenizer_path = f'{mbert_tokenizer_base_path}/{lang}/tokenizer/tokenizer.pkl'\n",
        "    with open(mbert_tokenizer_path, 'rb') as f:\n",
        "        mbert_tokenizers[lang] = pickle.load(f)\n",
        "\n",
        "    # Load the XLM tokenizer for this language\n",
        "    xlm_tokenizer_path = f'{xlm_tokenizer_base_path}/{lang}/tokenizer/tokenizer.pkl'\n",
        "    with open(xlm_tokenizer_path, 'rb') as f:\n",
        "        xlm_tokenizers[lang] = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "_o-KMd-ekPl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dictionaries to store the index_to_label mappings for all models and languages\n",
        "rembert_index_to_label = {}\n",
        "mbert_index_to_label = {}\n",
        "xlm_index_to_label = {}\n",
        "\n",
        "for lang in languages:\n",
        "    # Get the RemBERT tokenizer for this language\n",
        "    rembert_tokenizer = rembert_tokenizers[lang]\n",
        "\n",
        "    # Get the vocabulary from the RemBERT tokenizer\n",
        "    rembert_vocab = rembert_tokenizer.get_vocab()\n",
        "\n",
        "    # Create a dictionary that maps indices to tokens for the RemBERT tokenizer\n",
        "    rembert_index_to_label[lang] = {index: token for token, index in rembert_vocab.items()}\n",
        "\n",
        "    # Get the mBERT tokenizer for this language\n",
        "    mbert_tokenizer = mbert_tokenizers[lang]\n",
        "\n",
        "    # Get the vocabulary from the mBERT tokenizer\n",
        "    mbert_vocab = mbert_tokenizer.get_vocab()\n",
        "\n",
        "    # Create a dictionary that maps indices to tokens for the mBERT tokenizer\n",
        "    mbert_index_to_label[lang] = {index: token for token, index in mbert_vocab.items()}\n",
        "\n",
        "    # Get the XLM tokenizer for this language\n",
        "    xlm_tokenizer = xlm_tokenizers[lang]\n",
        "\n",
        "    # Get the vocabulary from the XLM tokenizer\n",
        "    xlm_vocab = xlm_tokenizer.get_vocab()\n",
        "\n",
        "    # Create a dictionary that maps indices to tokens for the XLM tokenizer\n",
        "    xlm_index_to_label[lang] = {index: token for token, index in xlm_vocab.items()}\n"
      ],
      "metadata": {
        "id": "bhj2O_4Tk-43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the base paths for the index_to_label data on Google Drive\n",
        "rembert_index_to_label_base_path = '/content/drive/MyDrive/RemBert_index_to_label'\n",
        "mbert_index_to_label_base_path = '/content/drive/MyDrive/mbert_index_to_label'\n",
        "xlm_index_to_label_base_path = '/content/drive/MyDrive/xlm_index_to_label'\n",
        "\n",
        "# Define the list of languages\n",
        "languages = ['EN-English', 'ES-Spanish', 'HI-Hindi', 'ZH-Chinese', 'SV-Swedish', 'FA-Farsi', 'FR-French', 'IT-Italian', 'PT-Portuguese', 'UK-Ukrainian', 'DE-German', 'BN-Bangla']\n",
        "\n",
        "# Create the directories for all models and languages if they don't already exist\n",
        "for lang in languages:\n",
        "    # Create the RemBERT index_to_label directory for this language\n",
        "    rembert_index_to_label_dir = f'{rembert_index_to_label_base_path}/{lang}'\n",
        "    if not os.path.exists(rembert_index_to_label_dir):\n",
        "        os.makedirs(rembert_index_to_label_dir)\n",
        "\n",
        "    # Create the mBERT index_to_label directory for this language\n",
        "    mbert_index_to_label_dir = f'{mbert_index_to_label_base_path}/{lang}'\n",
        "    if not os.path.exists(mbert_index_to_label_dir):\n",
        "        os.makedirs(mbert_index_to_label_dir)\n",
        "\n",
        "    # Create the XLM index_to_label directory for this language\n",
        "    xlm_index_to_label_dir = f'{xlm_index_to_label_base_path}/{lang}'\n",
        "    if not os.path.exists(xlm_index_to_label_dir):\n",
        "        os.makedirs(xlm_index_to_label_dir)\n",
        "\n",
        "\n",
        "# Save the index_to_label data for all models and languages\n",
        "for lang in languages:\n",
        "    # Save the RemBERT index_to_label data for this language\n",
        "    rembert_index_to_label_path = f'{rembert_index_to_label_base_path}/{lang}/index_to_label.pkl'\n",
        "    with open(rembert_index_to_label_path, 'wb') as f:\n",
        "        pickle.dump(rembert_index_to_label[lang], f)\n",
        "\n",
        "    # Save the mBERT index_to_label data for this language\n",
        "    mbert_index_to_label_path = f'{mbert_index_to_label_base_path}/{lang}/index_to_label.pkl'\n",
        "    with open(mbert_index_to_label_path, 'wb') as f:\n",
        "        pickle.dump(mbert_index_to_label[lang], f)\n",
        "\n",
        "    # Save the XLM index_to_label data for this language\n",
        "    xlm_index_to_label_path = f'{xlm_index_to_label_base_path}/{lang}/index_to_label.pkl'\n",
        "    with open(xlm_index_to_label_path, 'wb') as f:\n",
        "        pickle.dump(xlm_index_to_label[lang], f)\n"
      ],
      "metadata": {
        "id": "ZizRqy5ElCNC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}